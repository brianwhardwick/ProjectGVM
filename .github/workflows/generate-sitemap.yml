name: Auto-SEO & Sitemap

on:
  push:
    branches:
      - main
    paths:
      - '**/*.html'
      - '**/*.pdf'

permissions:
  contents: write

jobs:
  seo_and_sitemap:
    runs-on: ubuntu-latest
    name: Safe SEO Updates & Sitemap

    steps:
      - name: Checkout the repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v44
        with:
          files: |
            **.html

      - name: Run Safe SEO Script (Python)
        if: steps.changed-files.outputs.any_changed == 'true'
        run: |
          python3 -c "
          import os
          import re
          import json
          from datetime import datetime

          # --- CONFIGURATION ---
          BASE_URL = 'https://www.projectgvm.com.au'
          changed_files = '${{ steps.changed-files.outputs.all_changed_files }}'.split(' ')
          TODAY = datetime.now().strftime('%Y-%m-%d')

          def safe_replace(content, pattern, replacement):
              return re.sub(pattern, replacement, content)

          def update_meta_tag(content, property_name, new_value, is_name=False):
              # Handles both <meta property='...'> and <meta name='...'>
              attr = 'name' if is_name else 'property'
              # Sanitize new value for HTML
              safe_val = new_value.replace('\"', '&quot;')
              
              # Regex to find existing tag
              pattern = re.compile(f'(<meta\\s+{attr}=\"{property_name}\"\\s+content=\")(.*?)(\")', re.IGNORECASE)
              
              if pattern.search(content):
                  return pattern.sub(f'\\g<1>{safe_val}\\g<3>', content)
              else:
                  # If missing, we don't insert blindly to avoid breaking structure. 
                  # We only update existing tags for safety.
                  return content

          def process_file(file_path):
              if not os.path.exists(file_path): return
              
              print(f'Processing: {file_path}')
              with open(file_path, 'r', encoding='utf-8') as f:
                  original_content = f.read()

              content = original_content

              # 1. UPDATE SCHEMA DATES
              # Looks for \"dateModified\": \"YYYY-MM-DD\"
              content = re.sub(r'(\"dateModified\": \")(\d{4}-\d{2}-\d{2})(\")', f'\\g<1>{TODAY}\\g<3>', content)

              # 2. SYNC TITLE TO META TAGS
              title_match = re.search(r'<title>(.*?)</title>', content, re.IGNORECASE)
              if title_match:
                  page_title = title_match.group(1)
                  content = update_meta_tag(content, 'og:title', page_title)
                  content = update_meta_tag(content, 'twitter:title', page_title, is_name=True)
                  # Update Schema Headline
                  content = re.sub(r'(\"headline\": \")(.*?)(\")', f'\\g<1>{page_title}\\g<3>', content)

              # 3. SYNC DESCRIPTION TO META TAGS
              desc_match = re.search(r'<meta\s+name=\"description\"\s+content=\"(.*?)\"', content, re.IGNORECASE)
              if desc_match:
                  page_desc = desc_match.group(1)
                  content = update_meta_tag(content, 'og:description', page_desc)
                  content = update_meta_tag(content, 'twitter:description', page_desc, is_name=True)
                  # Update Schema Description
                  content = re.sub(r'(\"description\": \")(.*?)(\")', f'\\g<1>{page_desc}\\g<3>', content)

              # 4. BREADCRUMBS & CANONICALS (Only for non-root files)
              if file_path.endswith('index.html') and file_path != 'index.html':
                  # Calculate URL
                  parts = os.path.dirname(file_path).split('/')
                  if parts != ['']:
                      rel_path = '/'.join(parts) + '/'
                      full_url = f'{BASE_URL}/{rel_path}'
                      
                      # Update Canonical
                      content = re.sub(r'(<link\s+rel=\"canonical\"\s+href=\")(.*?)(\")', f'\\g<1>{full_url}\\g<3>', content)
                      # Update Schema @id
                      content = re.sub(r'(\"@id\": \")(.*?)(\")', f'\\g<1>{full_url}\\g<3>', content)

                      # Generate Breadcrumb JSON
                      breadcrumbs = {
                          '@context': 'https://schema.org',
                          '@type': 'BreadcrumbList',
                          'itemListElement': []
                      }
                      # Add Home
                      breadcrumbs['itemListElement'].append({'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': f'{BASE_URL}/'})
                      
                      # Add Parts
                      curr = ''
                      for i, part in enumerate(parts):
                          curr += part + '/'
                          name = part.replace('-', ' ').title()
                          if name.lower() == 'faq': name = 'FAQ'
                          if name.lower() == 'gvm gcm': name = 'GVM & GCM'
                          
                          breadcrumbs['itemListElement'].append({
                              '@type': 'ListItem', 
                              'position': i + 2, 
                              'name': name, 
                              'item': f'{BASE_URL}/{curr}'
                          })
                      
                      json_str = json.dumps(breadcrumbs, indent=2)
                      bc_block = f'\n    <script type=\"application/ld+json\">\n{json_str}\n    </script>\n    '

                      # Remove OLD Breadcrumb block if exists (Robust Regex)
                      content = re.sub(r'.*?\s*', '', content, flags=re.DOTALL)
                      
                      # Insert NEW Breadcrumb block before closing head
                      content = content.replace('</head>', f'{bc_block}\n</head>')

              # 5. SAFETY CHECK (CRITICAL)
              # If the file shrank by more than 50% or lost the closing head tag, DO NOT SAVE.
              if '</head>' not in content:
                  print(f'SAFETY ERROR: </head> tag missing in processed content for {file_path}. Skipping save.')
                  return
              
              if len(content) < (len(original_content) * 0.5):
                  print(f'SAFETY ERROR: Content shrank suspiciously for {file_path}. Skipping save.')
                  return

              # 6. WRITE FILE
              if content != original_content:
                  with open(file_path, 'w', encoding='utf-8') as f:
                      f.write(content)
                  print(f'Successfully updated {file_path}')

          # Run Loop
          for file in changed_files:
              if file.strip():
                  process_file(file)
          "

      - name: Generate Sitemap
        uses: cicirello/generate-sitemap@v1
        with:
          base-url-path: 'https://www.projectgvm.com.au'
          exclude-paths: |
            /assets/
            404.html
            /css/
            /js/
            CNAME
            robots.txt
          path-to-root: .
          include-html: true
          include-pdf: true

      - name: Commit and Push Changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Auto-SEO: Safe Python Updates + Sitemap [skip ci]"
